Service Verification Test Plan

All the verifications are already sorted in the priority of execution from Highest to Lowest. 
One can stop test execution at any time at any stage if one have no resources (time, env, engineers) to continue. 
But will see the risks of other (missed) verifications.


A. Preconditions (Installation):
  1. Docker image azshoo/alaska tag 1.0 could be successfully pulled from DockerHub
  2. Docker container azshoo/alaska:1.0 successfully starts

B. Post-Deployment Verification (PDV):
  3. Service responds with instruction text for HTTP GET request on http://0.0.0.0:8091, very basic functions (cleanup) work
    * Run PDV tests ('pdv' marker)

C. Testing (Functional):
  4. All available autotests have PASSED status (skipped tests, if any, are acknowledged)
    * Run smoke tests ('smoke' marker)
    * Run other non-SLOW tests ('not slow and not smoke' marker)
    * Run SLOW tests ('slow' marker)
  5. All available manual tests have PASSED status (skipped tests, if any, are acknowledged)
  6. Log messages during and after automatic and manual test executions have no CRIT and ERROR records (WARNING messages, if any, are acknowledged)
  7. Container RAM and CPU usage are about the same before and after tests (optional non-functional, for the case if part "D" will not be executed)

D. Testing (non-Functional) (could be started on a separate env right after PDV):
  8. Testing (Performance (Load)):
    * Service does not crash during 24h load test
    * Container RAM and CPU usage are about the same before and after several hours (24h) of load testing
    * Log messages have no CRIT and ERROR records (WARNING messages, if any, are acknowledged)
    * Disk space consumption during load tests is acceptable (logs rotation, temp and system files, etc.)
  9. Testing (Performance (Timings for clean system))
    * Response timings are about the same in the end of ramp-up period and at the end of load testing (no system degradation over test lifetime)
    * Response timings are about the same as in previous runs (if such runs exist)
  10. Testing (Performance (Timings for dirty system))
    * Response timings for every HTTP method on system with lots of records are acceptable
    * Response timings for every HTTP method on system with lots of records are about the same as in previous runs (if such runs exist)

E. Test Report
  11. Test Report is published and available for the team and concerned managers
